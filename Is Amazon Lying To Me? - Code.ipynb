{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db3617-3863-4efe-8865-c1f68c0f47e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/30 14:41:26 WARN Utils: Your hostname, Reehas-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.145.123.96 instead (on interface en0)\n",
      "25/11/30 14:41:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/30 14:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/30 14:41:30 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Volumes/One Touch/DMV_reviews/*.jsonl.\n",
      "java.io.FileNotFoundException: File /Volumes/One Touch/DMV_reviews/*.jsonl does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Volumes/One Touch/DMV_reviews/*.jsonl. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      9\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mAmazon Reviews\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     10\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m16g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     11\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.executor.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m16g\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     12\u001b[39m     .getOrCreate()\n\u001b[32m     13\u001b[39m data_path = \u001b[33m'\u001b[39m\u001b[33m/Volumes/One Touch/DMV_reviews/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df1 = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m*.jsonl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/pyspark/sql/readwriter.py:468\u001b[39m, in \u001b[36mDataFrameReader.json\u001b[39m\u001b[34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers, useUnsafeRow)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/Volumes/One Touch/DMV_reviews/*.jsonl. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "from pyspark.sql.functions import *\n",
    "import glob\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark with more memory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "data_path = '/Volumes/One Touch/DMV_reviews/'\n",
    "df1 = spark.read.json(f'{data_path}*.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb8c9f-9757-48bb-8873-916d7101d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the dataset\n",
    "print(f\"\\nSchema:\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9452c17-d792-494c-9075-3b6989d9fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "print(f\"Total rows: {df1.count():,}\")\n",
    "print(f\"Total columns: {len(df1.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9060e6-b55f-4578-bb54-e2778eb74367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb336bef-fcf8-4c95-a201-2d76ecca04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RATING DISTRIBUTION\")\n",
    "df1.groupBy('rating').count().orderBy('rating').show()\n",
    "\n",
    "print(\"VERIFIED PURCHASE DISTRIBUTION\")\n",
    "df1.groupBy('verified_purchase').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006dd4b-d73f-4069-8e45-2e468f9e047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Values\n",
    "\"\"\"\n",
    "It says no null values, however, we have columns such as Title, Text, and Images that will have null values unless the data collected was\n",
    "of reviews with title, text and images only. That isn't the case for this dataset, we get 0 null values simply because the columns are an\n",
    "empty string and hence not null.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "df1.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df1.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac42ef-c15d-4c9a-8b1c-58e8f08b3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('rating').count().orderBy('rating').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc89c6a-b3ca-4e59-8a8c-fadf0677720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REVIEWS WITH RATING = 0 (if any):\")\n",
    "df1.filter(col('rating') == 0.0).show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6cc01-9b8b-4c91-955a-a4788ca3c359",
   "metadata": {},
   "source": [
    "Okay, so from the above we can tell that the reviews with rating 0 are not empty. Additionally, Amazon's lowest rating is 1, not 0. This could indicate that there is an error. For example, the actual rating for item \"B00JYH2EHC\" is 2 stars (I know because I scrolled through all the ratings for that particular product on Amazon till I found a review left on August 22, 2018 with the Title -> No Good and Text -> Disappointed). Rows with the rating 0 are just 10, so it would make more sense just to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac30e3-9279-4219-a612-b8e3cb33300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for reviews in the range 1 to 5\n",
    "df1 = df1[df1['rating']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee6c2c-9268-4b99-95a6-05b1f6f5d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {df1.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47950437-f671-4ebe-9b63-0b7ffd56deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('rating').count().orderBy('rating').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e84e6-bd93-424b-8a68-a47f3371be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean = df1.dropDuplicates(['user_id', 'asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ac721-d0ab-40aa-952d-cdcc2104983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {df1_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f2187-d6cf-4678-b9da-dfa99c0d97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "data_path = '/Users/reehaalthaf/Downloads/DMV_metadata/'\n",
    "files = glob.glob(f'{data_path}*.jsonl')\n",
    "\n",
    "print(f\"Found {len(files)} metadata files\\n\")\n",
    "\n",
    "# Define schema manually to avoid reading 'details' field\n",
    "schema = StructType([\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"average_rating\", DoubleType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "])\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    try:\n",
    "        # Extract category from filename\n",
    "        filename = os.path.basename(file)  # e.g., \"meta_Electronics.jsonl\"\n",
    "        \n",
    "        # Remove \"meta_\" and \".jsonl\", then clean up\n",
    "        category_from_file = filename.replace('meta_', '') \\\n",
    "                                    .replace('.jsonl', '') \\\n",
    "                                    .replace('_', ' ') \\\n",
    "                                    .replace('...', ' ')  # Handle truncated names\n",
    "        \n",
    "        # Read file\n",
    "        df_temp = spark.read.schema(schema).json(file)\n",
    "        \n",
    "        # ADD COLUMN with filename category\n",
    "        df_temp = df_temp.withColumn('category_from_file', lit(category_from_file))\n",
    "        \n",
    "        dfs.append(df_temp)\n",
    "        print(f\"{data_path}: {df_temp.count():,} products â†’ category: '{category_from_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" {data_path}: {str(e)}\")\n",
    "\n",
    "        \n",
    "# Combine all\n",
    "if dfs:\n",
    "    df2 = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        df2 = df2.union(df)\n",
    "    \n",
    "    print(f\"\\n Total metadata loaded: {df2.count():,} products\")\n",
    "    \n",
    "    # Rename title to avoid conflict\n",
    "    df2 = df2.withColumnRenamed('title', 'product_title')\n",
    "    \n",
    "    print(\" Metadata ready!\")\n",
    "    df2.printSchema()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n No files loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfa6e7-e141-4ee4-8f21-95da082000fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {df2.count():,}\")\n",
    "print(f\"Total columns: {len(df2.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3f90b-9da7-4e07-b0a3-6358695801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df2.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f664ee-6766-4311-9365-e42119674d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_price = df2.filter(col('price').isNull())\n",
    "\n",
    "print(f\"Total products with missing price: {df_missing_price.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb1507-949d-41a1-92bf-1f9993a78ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_both_null = df2.filter(\n",
    "    col('price').isNull() & col('average_rating').isNull()\n",
    ").count()\n",
    "print(f\"Rows with BOTH null:  {rows_with_both_null:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e112db-b9ab-4812-b70e-c13353865b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_categories = df2.select('category_from_file').distinct().orderBy('category_from_file')\n",
    "print(f\"Total distinct categories: {distinct_categories.count()}\")\n",
    "distinct_categories.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d93df-54c0-4553-9e41-9755bddf6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We drop rows where the price values is null which would result in a majority of rows where average_rating are null values.\n",
    "To eliminate remaining rows where average_rating value is null we will drop those rows as well.\n",
    "\"\"\"\n",
    "df2_clean = df2.filter(\n",
    "    col('price').isNotNull() & \n",
    "    col('average_rating').isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"After:  {df2_clean.count():,} rows\")\n",
    "print(f\"Dropped: {df2.count() - df2_clean.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e106f8e-75bd-45d2-9d27-6d625a254fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df2_clean.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcf3fa-0cb5-47a1-9c89-bc44d174b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.filter(col('main_category').isNull()).show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f716d-195d-4a30-84c5-1c82c8d45ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716557c2-5448-4624-9c9b-7a9373350a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.write.mode('overwrite').parquet(\n",
    "    '/Users/reehaalthaf/Downloads/metadata_clean.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c671b-0e0b-4649-8cc5-d7a1fe1dcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean.write.mode('overwrite').parquet(\n",
    "    '/Users/reehaalthaf/Downloads/reviews_clean.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14e4c6-db92-477e-945d-3e7f77494cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Join the two dataframes on parent_asin\n",
    "combined = df1_clean.join(df2_clean, on='parent_asin', how='inner')\n",
    "\n",
    "print(f\" Combined dataset created!\")\n",
    "print(f\"Total rows: {combined.count():,}\")\n",
    "print(f\"Columns: {len(combined.columns)}\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nSchema:\")\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4312a9-816f-4043-b88d-6af7e70cf1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate after joining\n",
    "df_aggregated = combined.groupBy('parent_asin', 'price', 'main_category', 'category_from_file', 'user_id').agg(\n",
    "    # Verified reviews\n",
    "    avg(when(col('verified_purchase') == True, col('rating'))).alias('verified_avg_rating'),\n",
    "    count(when(col('verified_purchase') == True, 1)).alias('verified_count'),\n",
    "    \n",
    "    # Non-verified reviews\n",
    "    avg(when(col('verified_purchase') == False, col('rating'))).alias('nonverified_avg_rating'),\n",
    "    count(when(col('verified_purchase') == False, 1)).alias('nonverified_count'),\n",
    "    \n",
    "    # Overall\n",
    "    avg('rating').alias('overall_avg_rating'),\n",
    "    count('*').alias('total_reviews')\n",
    ").withColumn(\n",
    "    'trust_gap_stars',\n",
    "    col('nonverified_avg_rating') - col('verified_avg_rating')\n",
    ").withColumn(\n",
    "    'trust_gap_pct',\n",
    "    ((col('nonverified_avg_rating') - col('verified_avg_rating')) / col('verified_avg_rating') * 100)\n",
    ")\n",
    "\n",
    "# Filter to products with enough data\n",
    "df_aggregated = df_aggregated.filter(\n",
    "    (col('verified_count') >= 5) &  # At least 5 verified reviews\n",
    "    (col('nonverified_count') >= 5)  # At least 5 non-verified reviews\n",
    ")\n",
    "\n",
    "print(f\"Aggregated to {df_aggregated.count():,} products\")\n",
    "df_aggregated.show(10)\n",
    "\n",
    "df_aggregated.write.mode('overwrite').parquet(\n",
    "    '/Users/reehaalthaf/Downloads/aggregated_trust_gap.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa73800-2050-45c2-a771-325e20d1267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "df_final = df_aggregated.toPandas()\n",
    "\n",
    "# Save as CSV for Power BI\n",
    "df_final.to_csv('/Users/reehaalthaf/Downloads/trust_gap.csv', index=False)\n",
    "print(f\"Saved {len(df_final):,} rows to CSV\")\n",
    "print(f\"File size: ~{len(df_final) * 0.001:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e6999-0b72-45c0-b7ef-ec0860c7323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.approxQuantile('price', [0.25, 0.5, 0.75], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41df98d-bb0e-4210-85ca-778fe632ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated.select(\n",
    "    avg('verified_count').alias('avg_verified_per_product'),\n",
    "    avg('nonverified_count').alias('avg_nonverified_per_product'),\n",
    "    count('*').alias('total_products')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03116b6e-19b2-4d1d-b170-987cb7ae1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df1_clean.join(df2_clean, on='parent_asin', how='inner')\n",
    "# Aggregate after joining\n",
    "df_aggregated = combined.groupBy('parent_asin', 'price', 'main_category', 'category_from_file', 'user_id', 'store', 'timestamp', 'helpful_vote').agg(\n",
    "    # Verified reviews\n",
    "    avg(when(col('verified_purchase') == True, col('rating'))).alias('verified_avg_rating'),\n",
    "    count(when(col('verified_purchase') == True, 1)).alias('verified_count'),\n",
    "    \n",
    "    # Non-verified reviews\n",
    "    avg(when(col('verified_purchase') == False, col('rating'))).alias('nonverified_avg_rating'),\n",
    "    count(when(col('verified_purchase') == False, 1)).alias('nonverified_count'),\n",
    "    \n",
    "    # Overall\n",
    "    avg('rating').alias('overall_avg_rating'),\n",
    "    count('*').alias('total_reviews')\n",
    ").withColumn(\n",
    "    'trust_gap_stars',\n",
    "    col('nonverified_avg_rating') - col('verified_avg_rating')\n",
    ").withColumn(\n",
    "    'trust_gap_pct',\n",
    "    ((col('nonverified_avg_rating') - col('verified_avg_rating')) / col('verified_avg_rating') * 100)\n",
    ")\n",
    "\n",
    "# Filter to products with enough data\n",
    "df_aggregated = df_aggregated.filter(\n",
    "    (col('verified_count') >= 5) &  # At least 5 verified reviews\n",
    "    (col('nonverified_count') >= 5)  # At least 5 non-verified reviews\n",
    ")\n",
    "\n",
    "print(f\"Aggregated to {df_aggregated.count():,} products\")\n",
    "df_aggregated.show(10)\n",
    "\n",
    "df_aggregated.write.mode('overwrite').parquet(\n",
    "    '/Users/reehaalthaf/Downloads/aggregated_trust_gap.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112a3cc-0d28-418c-a13a-ad5b1e2ecef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check your data\n",
    "data_path = '/Volumes/One Touch/DMV_reviews/'\n",
    "files = glob.glob(f'{data_path}*.jsonl')\n",
    "\n",
    "print(f\"Number of files: {len(files)}\")\n",
    "print(f\"First few files:\")\n",
    "for f in files[:5]:\n",
    "    size = os.path.getsize(f) / (1024**3)  # Size in GB\n",
    "    print(f\"  {os.path.basename(f)}: {size:.2f} GB\")\n",
    "\n",
    "# Total size\n",
    "total_size = sum(os.path.getsize(f) for f in files) / (1024**3)\n",
    "print(f\"\\nTotal data size: {total_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0da89c-161f-483c-94ed-64d53dbd041b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
